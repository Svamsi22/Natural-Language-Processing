
# Natural Language Processing (NLP) Project

**Project Duration:** December 2023  
**Project Type:** Self Project

## Overview
This project focuses on text generation using Long Short-Term Memory (LSTM) neural networks. The goal was to establish a robust preprocessing and tokenization pipeline for large text datasets and achieve a high accuracy in text generation.

## Features
- **LSTM Neural Networks:** Utilized for text generation.
- **Preprocessing Pipeline:** Implemented robust preprocessing and tokenization for large text datasets.
- **High Accuracy:** Achieved an accuracy of 78.57%.
- **Comprehensive Project Management:** Oversaw the entire project lifecycle from data acquisition to model training and evaluation.
- **Large Dataset Management:** Managed over 100,000 textual documents, ensuring data quality through effective content curation strategies.

## Implementation Details
- **Language:** Python
- **Libraries:** TensorFlow, Keras, NumPy, Pandas, NLTK
- **Core Concepts:** LSTM Neural Networks, Text Generation, Data Preprocessing, Tokenization

## Getting Started
### Prerequisites
- Python 3.7 or higher
- Required Python libraries: TensorFlow, Keras, NumPy, Pandas, NLTK

### Installation
1. Clone the repository:
    ```bash
    git clone https://github.com/Svamsi22/NLP-Text-Generation-Project.git
    ```
2. Navigate to the project directory:
    ```bash
    cd NLP-Text-Generation-Project
    ```
3. Install the required libraries:
    ```bash
    pip install -r requirements.txt
    ```

### Usage
1. Preprocess the data:
    ```bash
    python preprocess.py
    ```
2. Train the LSTM model:
    ```bash
    python train.py
    ```
3. Generate text using the trained model:
    ```bash
    python generate.py
    ```

### Project Structure
- **data/**: Directory containing the dataset.
- **src/**
  - `preprocess.py`: Script for data preprocessing and tokenization.
  - `train.py`: Script for training the LSTM model.
  - `generate.py`: Script for generating text using the trained model.
- **models/**: Directory for saving trained models.
- **notebooks/**: Jupyter notebooks for experimentation and visualization.
- **requirements.txt**: List of required Python libraries.

## Results
The project achieved an accuracy of 78.57% in text generation, demonstrating the effectiveness of the preprocessing pipeline and the LSTM model.

## Contribution
Feel free to fork this repository, make enhancements, and submit pull requests. Contributions are always welcome!

## License
This project is licensed under the MIT License. See the `LICENSE` file for details.

## Acknowledgments
Special thanks to all the open-source contributors whose libraries and tools facilitated this project.

---

This project was a self-initiated effort to delve into the field of Natural Language Processing and explore the capabilities of LSTM neural networks for text generation.

---

