{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0f9663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sikha\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 18:57:49,074 - From C:\\Users\\sikha\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2024-09-11 18:57:49,140 - Starting model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\sikha\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 18:58:08,675 - From C:\\Users\\sikha\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sikha\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 18:58:11,411 - From C:\\Users\\sikha\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3476/3476 [==============================] - 551s 154ms/step - loss: 6.6362 - accuracy: 0.0554 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "3476/3476 [==============================] - 532s 153ms/step - loss: 6.1569 - accuracy: 0.0700 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "3476/3476 [==============================] - 392s 113ms/step - loss: 5.8790 - accuracy: 0.0854 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "3476/3476 [==============================] - 355s 102ms/step - loss: 5.5740 - accuracy: 0.1078 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "3476/3476 [==============================] - 338s 97ms/step - loss: 5.3006 - accuracy: 0.1245 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "3476/3476 [==============================] - 350s 101ms/step - loss: 5.0667 - accuracy: 0.1384 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "3476/3476 [==============================] - 359s 103ms/step - loss: 4.8607 - accuracy: 0.1476 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "3476/3476 [==============================] - 703s 202ms/step - loss: 4.6764 - accuracy: 0.1570 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "3476/3476 [==============================] - 492s 142ms/step - loss: 4.5073 - accuracy: 0.1670 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "3476/3476 [==============================] - 358s 103ms/step - loss: 4.3481 - accuracy: 0.1788 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "3476/3476 [==============================] - 351s 101ms/step - loss: 4.1960 - accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "3476/3476 [==============================] - 356s 103ms/step - loss: 4.0391 - accuracy: 0.2034 - lr: 9.5000e-04\n",
      "Epoch 13/50\n",
      "3476/3476 [==============================] - 359s 103ms/step - loss: 3.8878 - accuracy: 0.2197 - lr: 9.0250e-04\n",
      "Epoch 14/50\n",
      "3476/3476 [==============================] - 380s 109ms/step - loss: 3.7420 - accuracy: 0.2370 - lr: 8.5737e-04\n",
      "Epoch 15/50\n",
      "3476/3476 [==============================] - 374s 108ms/step - loss: 3.6075 - accuracy: 0.2560 - lr: 8.1451e-04\n",
      "Epoch 16/50\n",
      "3476/3476 [==============================] - 370s 106ms/step - loss: 3.4791 - accuracy: 0.2729 - lr: 7.7378e-04\n",
      "Epoch 17/50\n",
      "3476/3476 [==============================] - 385s 111ms/step - loss: 3.3557 - accuracy: 0.2913 - lr: 7.3509e-04\n",
      "Epoch 18/50\n",
      "3476/3476 [==============================] - 365s 105ms/step - loss: 3.2419 - accuracy: 0.3107 - lr: 6.9834e-04\n",
      "Epoch 19/50\n",
      "3476/3476 [==============================] - 350s 101ms/step - loss: 3.1326 - accuracy: 0.3274 - lr: 6.6342e-04\n",
      "Epoch 20/50\n",
      "3476/3476 [==============================] - 360s 103ms/step - loss: 3.0295 - accuracy: 0.3444 - lr: 6.3025e-04\n",
      "Epoch 21/50\n",
      "3476/3476 [==============================] - 475s 137ms/step - loss: 2.9301 - accuracy: 0.3608 - lr: 5.9874e-04\n",
      "Epoch 22/50\n",
      "3476/3476 [==============================] - 396s 114ms/step - loss: 2.8369 - accuracy: 0.3784 - lr: 5.6880e-04\n",
      "Epoch 23/50\n",
      "3476/3476 [==============================] - 357s 103ms/step - loss: 2.7464 - accuracy: 0.3947 - lr: 5.4036e-04\n",
      "Epoch 24/50\n",
      "3476/3476 [==============================] - 439s 126ms/step - loss: 2.6626 - accuracy: 0.4101 - lr: 5.1334e-04\n",
      "Epoch 25/50\n",
      "3476/3476 [==============================] - 548s 158ms/step - loss: 2.5816 - accuracy: 0.4259 - lr: 4.8767e-04\n",
      "Epoch 26/50\n",
      "3476/3476 [==============================] - 552s 159ms/step - loss: 2.5039 - accuracy: 0.4412 - lr: 4.6329e-04\n",
      "Epoch 27/50\n",
      "3476/3476 [==============================] - 551s 159ms/step - loss: 2.4315 - accuracy: 0.4565 - lr: 4.4013e-04\n",
      "Epoch 28/50\n",
      "3476/3476 [==============================] - 528s 152ms/step - loss: 2.3610 - accuracy: 0.4705 - lr: 4.1812e-04\n",
      "Epoch 29/50\n",
      "3476/3476 [==============================] - 461s 133ms/step - loss: 2.2941 - accuracy: 0.4854 - lr: 3.9721e-04\n",
      "Epoch 30/50\n",
      "3476/3476 [==============================] - 461s 133ms/step - loss: 2.2302 - accuracy: 0.4981 - lr: 3.7735e-04\n",
      "Epoch 31/50\n",
      "3476/3476 [==============================] - 489s 141ms/step - loss: 2.1690 - accuracy: 0.5117 - lr: 3.5849e-04\n",
      "Epoch 32/50\n",
      "3476/3476 [==============================] - 487s 140ms/step - loss: 2.1109 - accuracy: 0.5253 - lr: 3.4056e-04\n",
      "Epoch 33/50\n",
      "3476/3476 [==============================] - 496s 143ms/step - loss: 2.0560 - accuracy: 0.5367 - lr: 3.2353e-04\n",
      "Epoch 34/50\n",
      "3476/3476 [==============================] - 499s 143ms/step - loss: 2.0032 - accuracy: 0.5493 - lr: 3.0736e-04\n",
      "Epoch 35/50\n",
      "3476/3476 [==============================] - 424s 122ms/step - loss: 1.9542 - accuracy: 0.5607 - lr: 2.9199e-04\n",
      "Epoch 36/50\n",
      "3476/3476 [==============================] - 331s 95ms/step - loss: 1.9061 - accuracy: 0.5705 - lr: 2.7739e-04\n",
      "Epoch 37/50\n",
      "3476/3476 [==============================] - 339s 98ms/step - loss: 1.8608 - accuracy: 0.5817 - lr: 2.6352e-04\n",
      "Epoch 38/50\n",
      "3476/3476 [==============================] - 345s 99ms/step - loss: 1.8171 - accuracy: 0.5921 - lr: 2.5034e-04\n",
      "Epoch 39/50\n",
      "3476/3476 [==============================] - 341s 98ms/step - loss: 1.7775 - accuracy: 0.6008 - lr: 2.3783e-04\n",
      "Epoch 40/50\n",
      "3476/3476 [==============================] - 464s 133ms/step - loss: 1.7383 - accuracy: 0.6114 - lr: 2.2594e-04\n",
      "Epoch 41/50\n",
      "3476/3476 [==============================] - 455s 131ms/step - loss: 1.7014 - accuracy: 0.6206 - lr: 2.1464e-04\n",
      "Epoch 42/50\n",
      "3476/3476 [==============================] - 324s 93ms/step - loss: 1.6657 - accuracy: 0.6280 - lr: 2.0391e-04\n",
      "Epoch 43/50\n",
      "3476/3476 [==============================] - 339s 98ms/step - loss: 1.6334 - accuracy: 0.6368 - lr: 1.9371e-04\n",
      "Epoch 44/50\n",
      "3476/3476 [==============================] - 497s 143ms/step - loss: 1.6016 - accuracy: 0.6446 - lr: 1.8403e-04\n",
      "Epoch 45/50\n",
      "3476/3476 [==============================] - 393s 113ms/step - loss: 1.5716 - accuracy: 0.6510 - lr: 1.7482e-04\n",
      "Epoch 46/50\n",
      "3476/3476 [==============================] - 348s 100ms/step - loss: 1.5430 - accuracy: 0.6594 - lr: 1.6608e-04\n",
      "Epoch 47/50\n",
      "3476/3476 [==============================] - 343s 99ms/step - loss: 1.5161 - accuracy: 0.6650 - lr: 1.5778e-04\n",
      "Epoch 48/50\n",
      "3476/3476 [==============================] - 346s 100ms/step - loss: 1.4904 - accuracy: 0.6722 - lr: 1.4989e-04\n",
      "Epoch 49/50\n",
      "3476/3476 [==============================] - 354s 102ms/step - loss: 1.4666 - accuracy: 0.6782 - lr: 1.4240e-04\n",
      "Epoch 50/50\n",
      "3476/3476 [==============================] - 353s 102ms/step - loss: 1.4433 - accuracy: 0.6844 - lr: 1.3528e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 00:45:00,508 - Model saved at models/word_generate_model_v1.1.h5\n",
      "2024-09-12 00:45:00,708 - Tokenizer saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: This is became improved since i came down under this morning and\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Setup logging for debugging and performance tracking\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Specify the path to your text file\n",
    "file_path = r\"1661-0 (1).txt\"\n",
    "\n",
    "# Step 1: Load and preprocess the text data with exception handling\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        book_text = file.read()\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"File not found: {e}\")\n",
    "    raise\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([book_text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences and labels\n",
    "input_sequences = tokenizer.texts_to_sequences([book_text])[0]\n",
    "max_sequence_length = 50  # For example\n",
    "\n",
    "sequences = []\n",
    "for i in range(1, len(input_sequences)):\n",
    "    if i >= max_sequence_length:\n",
    "        n_gram_sequence = input_sequences[i - max_sequence_length : i + 1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "# Ensure that X has the correct shape\n",
    "input_sequence_length = max_sequence_length - 1\n",
    "X = pad_sequences(X, maxlen=input_sequence_length, padding='pre')\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Step 2: Dynamic learning rate scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    return lr * 0.95 if epoch > 10 else lr\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(input_sequence_length,))\n",
    "\n",
    "# Step 3: Embedding and LSTM layers\n",
    "embedding_layer = Embedding(total_words, 100, input_length=input_sequence_length)(input_layer)\n",
    "lstm_output = LSTM(150, return_sequences=True)(embedding_layer)\n",
    "\n",
    "# Step 4: Attention mechanism (query = value = lstm_output)\n",
    "attention = Attention()([lstm_output, lstm_output])  # Use the same LSTM output as both query and value\n",
    "\n",
    "# Concatenate the attention output and LSTM output\n",
    "concatenated = Concatenate()([lstm_output, attention])\n",
    "\n",
    "# Step 5: Another LSTM layer and Dense output\n",
    "lstm_output_2 = LSTM(100)(concatenated)\n",
    "output_layer = Dense(total_words, activation='softmax')(lstm_output_2)\n",
    "\n",
    "# Step 6: Create and compile the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Add learning rate scheduler\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Step 7: Train the model with callbacks and logging\n",
    "logging.info(\"Starting model training...\")\n",
    "model.fit(X, y, epochs=50, verbose=1, callbacks=[lr_scheduler])\n",
    "\n",
    "# Step 8: Save model with versioning and metadata\n",
    "model_version = \"v1.1\"\n",
    "model_dir = f'models/word_generate_model_{model_version}.h5'\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "model.save_weights(model_dir)\n",
    "logging.info(f\"Model saved at {model_dir}\")\n",
    "\n",
    "# Save model tokenizer for future predictions\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(f'tokenizer_{model_version}.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "logging.info(\"Tokenizer saved.\")\n",
    "\n",
    "# Step 9: Generate text with better prediction using beam search\n",
    "def beam_search(seed_text, num_words, model, max_sequence_length, beam_width=3):\n",
    "    sequences = [[seed_text, 0.0]]\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            token_list = tokenizer.texts_to_sequences([seq])[0]\n",
    "            token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "            predicted_probabilities = model.predict(token_list, verbose=0)[0]\n",
    "            \n",
    "            # Consider top beam_width predictions\n",
    "            top_predictions = np.argsort(predicted_probabilities)[-beam_width:]\n",
    "            \n",
    "            for pred in top_predictions:\n",
    "                new_seq = seq + ' ' + tokenizer.index_word[pred]\n",
    "                new_score = score - np.log(predicted_probabilities[pred])\n",
    "                all_candidates.append([new_seq, new_score])\n",
    "        \n",
    "        # Select top beam_width sequences with the highest score\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
    "    \n",
    "    return sequences[0][0]\n",
    "\n",
    "# Generate text using the advanced beam search method\n",
    "generated_text = beam_search(\"This is\", 10, model, max_sequence_length)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af435c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
